{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGOAL: Use more data for language and non-language (not restrited to visual)\\n      Optimize RandomForest paramter using GridSearchCV\\n      Keep reducing channels with lowest feature importance for each classification until score\\n          not improve.\\n      PCA the remaining channels than classify and see results\\n      Plot the feature importance of top 10% consistent channels across folds\\n      OPTIONAL: Do shotgun approach.\\n      \\n'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "GOAL: Use more data for language and non-language (not restrited to visual)\n",
    "      Optimize RandomForest paramter using GridSearchCV\n",
    "      Keep reducing channels with lowest feature importance for each classification until score\n",
    "          not improve.\n",
    "      PCA the remaining channels than classify and see results\n",
    "      Plot the feature importance of top 10% consistent channels across folds\n",
    "      OPTIONAL: Do shotgun approach.\n",
    "      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classify function\n",
    "\"\"\"\n",
    "\n",
    "def classifier(features,target):\n",
    "    clf = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "    #clf = LogisticRegression()\n",
    "    cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "    scores = []\n",
    "    \n",
    "    for train,test in cv.split(features,target):\n",
    "        preds = []\n",
    "        X_train = features[train]; y_train = target[train] \n",
    "        X_test = features[test];   y_test = target[test]\n",
    "\n",
    "    \n",
    "\n",
    "        twoD_X_train = Vectorizer().fit_transform(X_train)\n",
    "        #print('original shape of training set',(X_train.shape))\n",
    "        #print('Vectorized shape of training set',(twoD_X_train.shape))\n",
    "        clf.fit(twoD_X_train,y_train)\n",
    "\n",
    "        #print(clf.feature_importances_.shape)\n",
    "        twoD_X_test = Vectorizer().fit_transform(X_test)\n",
    "        #print('original shape of test set',(X_test.shape))\n",
    "        #print('Vectorized shape of test set',(twoD_X_test.shape))\n",
    "        \n",
    "        preds.append(clf.predict(twoD_X_test))\n",
    "        #print('shape of y_test is: ',np.array(y_test).shape)\n",
    "        #print(np.array(preds).shape)\n",
    "        d = np.transpose(preds).ravel()\n",
    "        #print('shape of pred is: ',d.shape)\n",
    "        \n",
    "        print('Accuracy on training set {:.3f}'.format(clf.score(twoD_X_train,y_train)))\n",
    "        print('Accuracy on test set {:.3f}'.format(clf.score(twoD_X_test,y_test)))\n",
    "        scores.append(clf.score(twoD_X_test,y_test))\n",
    "        \n",
    "        target_names = ['lang', 'non-lang']\n",
    "\n",
    "        \n",
    "        report = classification_report(y_test, d, target_names=target_names)\n",
    "        print(report)\n",
    "        \n",
    "    print('standard deviation of scores: ',np.std(scores))    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reorganize features importance array by sum per three?\n",
    "\n",
    "\"\"\"\n",
    "def importantChannelFinder(features,target):\n",
    "    forest = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "    #clf = LogisticRegression()\n",
    "    #cv = StratifiedKFold(n_splits=1,shuffle=True)\n",
    "    scores = []\n",
    "    \"\"\"creating channel dictionary\"\"\"\n",
    "    ch_dict = {}\n",
    "    for i in range(0,125):\n",
    "        ch_dict[i] = 0\n",
    "    #print(ch_dict)    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.25, random_state=42)\n",
    "        \n",
    "    twoD_X_train = Vectorizer().fit_transform(X_train)\n",
    "    forest.fit(twoD_X_train,y_train)\n",
    "\n",
    "    \"\"\"manipulate array to 125 features\"\"\"\n",
    "    impt = []\n",
    "    for e in forest.feature_importances_:\n",
    "        impt.append(e)\n",
    "\n",
    "    print(\"shape of important feat array: \",np.array(impt).shape)\n",
    "#         temp = []\n",
    "#         time = binary_epoch.get_data().shape[2]\n",
    "#         channel_time = binary_epoch.get_data().shape[1] * binary_epoch.get_data().shape[2]\n",
    "#         for i in range(0,channel_time,time):\n",
    "#             #print(i)\n",
    "#             hold = []\n",
    "#             for j in range(i,i+time):\n",
    "#                 hold.append(impt[j])\n",
    "\n",
    "#             temp.append(np.sum(hold))    \n",
    "\n",
    "#         #print(\"new shape of important feat array: \",np.array(temp).shape) \n",
    "#         a = np.array(temp)\n",
    "#         ind = np.argpartition(a, -20)[-20:]  #top ten\n",
    "#         print(\"top twenty channels: \",ind)\n",
    "#         print('top twenty channels importances: ',a[ind])\n",
    "        \n",
    "#         for i in ind:\n",
    "#             value = ch_dict[i]\n",
    "#             ch_dict[i] = value + 1 \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "# #         twoD_X_test = Vectorizer().fit_transform(X_test)\n",
    "        \n",
    "        \n",
    "# #         preds.append(forest.predict(twoD_X_test))\n",
    "        \n",
    "# #         d = np.transpose(preds).ravel()\n",
    "        \n",
    "# #         scores.append(forest.score(twoD_X_test,y_test))\n",
    "        \n",
    "# #         target_names = ['lang', 'non-lang']\n",
    "# #         report = classification_report(y_test, d, target_names=target_names)\n",
    "#         #print(report)\n",
    "        \n",
    "#     for kk in ch_dict:\n",
    "#         v = master[kk]\n",
    "#         master[kk] = v + ch_dict[kk]\n",
    "    \n",
    "#     hold = [] #keys\n",
    "#     hold2 = [] #values\n",
    "#     for key in ch_dict:\n",
    "#         if ch_dict[key] >= 5:\n",
    "#             hold.append(key)\n",
    "#             hold2.append(ch_dict[key])\n",
    "#     print('importanct channels shape: ',np.array(hold).shape)        \n",
    "#     plt.bar(hold,hold2) \n",
    "#     hold3 = []\n",
    "#     for k in hold:\n",
    "#         if binary_epoch.ch_names[k] != 'STI 014':\n",
    "#             hold3.append(binary_epoch.ch_names[k])\n",
    "\n",
    "#     print(hold3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter Epoch data for language vs non-language\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "stim_combinations = {\n",
    "    (5,6,4): \"AALL\",\n",
    "    (17,18,16) :\"AALL\",\n",
    "    \n",
    "    (11,6,10):\"AALN\",\n",
    "    (23,18,22):\"AALN\",\n",
    "    \n",
    "    (5,12,4) :\"AANL\",\n",
    "    (17,24,16):\"AANL\",\n",
    "    \n",
    "    (11,12,10) : \"AANN\",\n",
    "    (23,24,22): \"AANN\",\n",
    "    \n",
    "    (2,6,1):\"AVLL\",\n",
    "    (14,18,13):\"AVLL\",\n",
    "    \n",
    "    (8,12,7):\"AVNN\",\n",
    "    (20,24,19):\"AVNN\",\n",
    "    \n",
    "    (5,3,4) :\"VALL\",\n",
    "    (17,15,16):\"VALL\",\n",
    "    \n",
    "    (11,9,10) :\"VANN\",\n",
    "    (23,21,22):\"VANN\",\n",
    "    \n",
    "    (2,3,1) :\"VVLL\",\n",
    "    (14,15,13):\"VVLL\",\n",
    "    \n",
    "    (8,3,7):\"VVLN\",\n",
    "    (20,15,19):\"VVLN\",\n",
    "    \n",
    "    (2,9,1) :\"VVNL\",\n",
    "    (14,21,13):\"VVNL\",\n",
    "    \n",
    "    (8,9,7) :\"VVNN\",\n",
    "    (20,21,19):\"VVNN\",\n",
    "    \n",
    "    (14,21,4):\"whatever\"\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Function detects stim combos and changes their event id to 100 for language and 200 for non-l\n",
    "\"\"\"\n",
    "def find_stim_combo(epochObj,stimCombos1,stimCombos2):\n",
    "    events = epochObj.events\n",
    "    numOfEvents = epochObj.events.shape[0]\n",
    "    #print('shape of events: ',numOfEvents)\n",
    "    \n",
    "    events_new = []\n",
    "    combo = []\n",
    "    \n",
    "    for i in range(0,numOfEvents,3):\n",
    "        temp = []\n",
    "        #print('three')\n",
    "        for h in range(i,i+3):\n",
    "            #print(events[h][-1])\n",
    "            combo.append(events[h])\n",
    "            \n",
    "        combo_tuple = (combo[0][-1],combo[1][-1],combo[2][-1])   \n",
    "        #print(combo_tuple)\n",
    "        for k in stimCombos1:\n",
    "            if k == stim_combinations[combo_tuple]:\n",
    "                #print('found combo')\n",
    "                combo[0][-1] = 100\n",
    "                combo[1][-1] = 100\n",
    "                combo[2][-1] = 100\n",
    "                temp.append(combo[0])\n",
    "                temp.append(combo[1])\n",
    "                temp.append(combo[2])\n",
    "        \n",
    "        for j in stimCombos2:\n",
    "            if j == stim_combinations[combo_tuple]:\n",
    "                #print('found combo')\n",
    "                combo[0][-1] = 200\n",
    "                combo[1][-1] = 200\n",
    "                combo[2][-1] = 200\n",
    "                temp.append(combo[0])\n",
    "                temp.append(combo[1])\n",
    "                temp.append(combo[2])        \n",
    "        #print(temp)\n",
    "        for p in temp:\n",
    "            events_new.append(p)\n",
    "        combo = []\n",
    "    print(\"total events b4 filtering: \",len(events))    \n",
    "    print(\"total events aftr filtering: \",len(events_new)) \n",
    "    return events_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do(epoch):\n",
    "    #epoch = mne.read_epochs(p,preload=True)\n",
    "    epoch.drop_channels(ch_names=[ 'LL4', 'L12'])\n",
    "    epoch.drop_channels(ch_names=[ 'Nasium', 'VEOG'])\n",
    "    print('dropped channels LL4,L12,NASIUM,VEOG')\n",
    "    \n",
    "    epoch_copy = epoch.copy()\n",
    "    \n",
    "    languageStimCodes = ['AALL','AVLL','VVLL']\n",
    "    nonLanguageStimCodes = ['AANN','AVNN','VVNN']\n",
    "    \n",
    "    find_stim_combo(epoch_copy,languageStimCodes,nonLanguageStimCodes)\n",
    "    binary_epoch = epoch_copy[(epoch_copy.events[:,-1]==100) | (epoch_copy.events[:,-1]==200)].copy()\n",
    "    print('shape of epoch after filtered events:',binary_epoch.get_data().shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    Classify filtered data and check results\n",
    "    \"\"\"\n",
    "    features = binary_epoch.get_data()\n",
    "    print('shape of features:',features.shape)\n",
    "    target = binary_epoch.events[:,-1]\n",
    "    print('shape of target:',target.shape)\n",
    "    \n",
    "    #print(\"mean classification score\", classifier(features,target))\n",
    "    \n",
    "    importantChannelFinder(features,target)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubjFolderSearch(path):\n",
    "    subjFolders = os.listdir(path)\n",
    "    AllFiles = []\n",
    "    for i in subjFolders:\n",
    "        if '.DS_Store' not in i:\n",
    "            AllFiles.append(os.path.join(path,i))\n",
    "    \n",
    "    \n",
    "    return AllFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Volumes/flash64/fifFiles/20140205_1114_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "1908 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "1908 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "path = '/Volumes/flash64/fifFiles'\n",
    "AllSetFiles = SubjFolderSearch(path)\n",
    "#print(AllSetFiles)\n",
    "\n",
    "epo = mne.read_epochs(AllSetFiles[2])\n",
    "\n",
    "\n",
    "# master = {}\n",
    "# for i in range(0,125):\n",
    "#     master[i] = 0\n",
    "    \n",
    "# def find_mod3files(fileArray):\n",
    "#     #good_paths = []\n",
    "#     for file in fileArray:\n",
    "#         epo = mne.read_epochs(file)\n",
    "#         if epo.get_data().shape[0] % 3 == 0:\n",
    "#             do(epo)\n",
    "#             break;\n",
    "\n",
    "# find_mod3files(AllSetFiles)\n",
    "\n",
    "# h1 = []\n",
    "# h2 = []\n",
    "# for k in master:\n",
    "#     h1.append(k)\n",
    "#     h2.append(master[k])\n",
    "    \n",
    "# plt.bar(h1,h2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped channels LL4,L12,NASIUM,VEOG\n",
      "total events b4 filtering:  1908\n",
      "total events aftr filtering:  957\n",
      "shape of epoch after filtered events: (957, 125, 257)\n",
      "shape of features: (957, 125, 257)\n",
      "shape of target: (957,)\n",
      "shape of important feat array:  (32125,)\n"
     ]
    }
   ],
   "source": [
    "epo_copy = epo.copy();\n",
    "do(epo_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
